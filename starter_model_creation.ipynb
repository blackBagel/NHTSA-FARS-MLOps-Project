{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import recall_score\n",
    "import mlflow\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper, gen_features\n",
    "from sklearn.base import TransformerMixin\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What will happen to you if you were in a car accident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "\n",
    "# with open('./data/accident.CSV', 'rb') as filedata:\n",
    "#     result = chardet.detect(filedata.read(10000000))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1922/611547901.py:1: DtypeWarning: Columns (15,106,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./data/FARS2019NationalCSV/Person.CSV', encoding='Windows-1252')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATENAME</th>\n",
       "      <th>ST_CASE</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>VEH_NO</th>\n",
       "      <th>PER_NO</th>\n",
       "      <th>STR_VEH</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAYNAME</th>\n",
       "      <th>...</th>\n",
       "      <th>WORK_INJ</th>\n",
       "      <th>WORK_INJNAME</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>HISPANICNAME</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>LOCATIONNAME</th>\n",
       "      <th>HELM_USE</th>\n",
       "      <th>HELM_USENAME</th>\n",
       "      <th>HELM_MIS</th>\n",
       "      <th>HELM_MISNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Applicable (not a fatality)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not A Fatality (not Applicable)</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Applicable (not a fatality)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not A Fatality (not Applicable)</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10002</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10002</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Applicable (not a fatality)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not A Fatality (not Applicable)</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE STATENAME  ST_CASE  VE_FORMS  VEH_NO  PER_NO  STR_VEH  COUNTY  DAY  \\\n",
       "0      1   Alabama    10001         2       1       1        0      81    7   \n",
       "1      1   Alabama    10001         2       1       2        0      81    7   \n",
       "2      1   Alabama    10001         2       2       1        0      81    7   \n",
       "3      1   Alabama    10002         2       1       1        0      55   23   \n",
       "4      1   Alabama    10002         2       2       1        0      55   23   \n",
       "\n",
       "   DAYNAME  ...  WORK_INJ                     WORK_INJNAME  HISPANIC  \\\n",
       "0        7  ...         8  Not Applicable (not a fatality)         0   \n",
       "1        7  ...         0                               No         7   \n",
       "2        7  ...         8  Not Applicable (not a fatality)         0   \n",
       "3       23  ...         0                               No         7   \n",
       "4       23  ...         8  Not Applicable (not a fatality)         0   \n",
       "\n",
       "                      HISPANICNAME  LOCATION                 LOCATIONNAME  \\\n",
       "0  Not A Fatality (not Applicable)         0  Occupant of a Motor Vehicle   \n",
       "1                     Non-Hispanic         0  Occupant of a Motor Vehicle   \n",
       "2  Not A Fatality (not Applicable)         0  Occupant of a Motor Vehicle   \n",
       "3                     Non-Hispanic         0  Occupant of a Motor Vehicle   \n",
       "4  Not A Fatality (not Applicable)         0  Occupant of a Motor Vehicle   \n",
       "\n",
       "   HELM_USE    HELM_USENAME  HELM_MIS              HELM_MISNAME  \n",
       "0        20  Not Applicable         7  None Used/Not Applicable  \n",
       "1        20  Not Applicable         7  None Used/Not Applicable  \n",
       "2        20  Not Applicable         7  None Used/Not Applicable  \n",
       "3        20  Not Applicable         7  None Used/Not Applicable  \n",
       "4        20  Not Applicable         7  None Used/Not Applicable  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/FARS2019NationalCSV/Person.CSV', encoding='Windows-1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['STATE', 'STATENAME', 'ST_CASE', 'VE_FORMS', 'VEH_NO', 'PER_NO',\n",
       "       'STR_VEH', 'COUNTY', 'DAY', 'DAYNAME', 'MONTH', 'MONTHNAME',\n",
       "       'HOUR', 'HOURNAME', 'MINUTE', 'MINUTENAME', 'RUR_URB',\n",
       "       'RUR_URBNAME', 'FUNC_SYS', 'FUNC_SYSNAME', 'HARM_EV',\n",
       "       'HARM_EVNAME', 'MAN_COLL', 'MAN_COLLNAME', 'SCH_BUS',\n",
       "       'SCH_BUSNAME', 'MAKE', 'MAKENAME', 'MAK_MOD', 'BODY_TYP',\n",
       "       'BODY_TYPNAME', 'MOD_YEAR', 'MOD_YEARNAME', 'TOW_VEH',\n",
       "       'TOW_VEHNAME', 'SPEC_USE', 'SPEC_USENAME', 'EMER_USE',\n",
       "       'EMER_USENAME', 'ROLLOVER', 'ROLLOVERNAME', 'IMPACT1',\n",
       "       'IMPACT1NAME', 'FIRE_EXP', 'FIRE_EXPNAME', 'AGE', 'AGENAME', 'SEX',\n",
       "       'SEXNAME', 'PER_TYP', 'PER_TYPNAME', 'INJ_SEV', 'INJ_SEVNAME',\n",
       "       'SEAT_POS', 'SEAT_POSNAME', 'REST_USE', 'REST_USENAME', 'REST_MIS',\n",
       "       'REST_MISNAME', 'AIR_BAG', 'AIR_BAGNAME', 'EJECTION',\n",
       "       'EJECTIONNAME', 'EJ_PATH', 'EJ_PATHNAME', 'EXTRICAT',\n",
       "       'EXTRICATNAME', 'DRINKING', 'DRINKINGNAME', 'ALC_DET',\n",
       "       'ALC_DETNAME', 'ALC_STATUS', 'ALC_STATUSNAME', 'ATST_TYP',\n",
       "       'ATST_TYPNAME', 'ALC_RES', 'ALC_RESNAME', 'DRUGS', 'DRUGSNAME',\n",
       "       'DRUG_DET', 'DRUG_DETNAME', 'DSTATUS', 'DSTATUSNAME', 'HOSPITAL',\n",
       "       'HOSPITALNAME', 'DOA', 'DOANAME', 'DEATH_DA', 'DEATH_DANAME',\n",
       "       'DEATH_MO', 'DEATH_MONAME', 'DEATH_YR', 'DEATH_YRNAME', 'DEATH_HR',\n",
       "       'DEATH_HRNAME', 'DEATH_MN', 'DEATH_MNNAME', 'DEATH_TM',\n",
       "       'DEATH_TMNAME', 'LAG_HRS', 'LAG_HRSNAME', 'LAG_MINS',\n",
       "       'LAG_MINSNAME', 'P_SF1', 'P_SF1NAME', 'P_SF2', 'P_SF2NAME',\n",
       "       'P_SF3', 'P_SF3NAME', 'WORK_INJ', 'WORK_INJNAME', 'HISPANIC',\n",
       "       'HISPANICNAME', 'LOCATION', 'LOCATIONNAME', 'HELM_USE',\n",
       "       'HELM_USENAME', 'HELM_MIS', 'HELM_MISNAME'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVESTIGATED_COLUMNS = [\n",
    "    'ST_CASE', \n",
    "    'STATE',\n",
    "    'STATENAME',\n",
    "    'VEH_NO',\n",
    "    'VE_FORMS',\n",
    "    'PER_NO',\n",
    "    'COUNTY',\n",
    "    'DAY',\n",
    "    'MONTH',\n",
    "    'HOUR',\n",
    "    'AGE',\n",
    "    'SEX',\n",
    "    'INJ_SEV',\n",
    "    'INJ_SEVNAME',\n",
    "    'DOA',\n",
    "    'DOANAME',\n",
    "    'SEAT_POS',\n",
    "    'REST_USE',\n",
    "]\n",
    "TARGET = 'INJ_SEV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "df = df[INVESTIGATED_COLUMNS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping irrelevant injury categories\n",
    "\n",
    "The dataset user manual states these are the possible values for the injury severity field (`INJ_SEV`):\n",
    "\n",
    "- 0 - No Apparent Injury\n",
    "- 1 - Possible Injury\n",
    "- 2 - Suspected Minor Injury\n",
    "- 3 - Suspected Serious Injury\n",
    "- 4 - Fatal Injury \n",
    "- 5 - Injured, Severity Unknown\n",
    "- 6 - Died Prior to Crash\n",
    "- 9 - Unknown/Not Reported \n",
    "\n",
    "Since we want to teach our model to predict a specific injury severity, we'll only use categories 0-4\n",
    "\n",
    "In addition, we'll consider death as another type of injury. </br>\n",
    "The user manual describes the column detailing death (`DOA`) as such:\n",
    "\n",
    "- 0 Not Applicable \n",
    "- 7 Died at Scene\n",
    "- 8 Died En Route (to a hospital)\n",
    "- 9 Unknown\n",
    "\n",
    "Again, we'll ignore cases where death is unknown and focus on categories 0,7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['INJ_SEV'] <= 4) & (df['DOA'] != 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to combine the injury severity and death outcome results to one field, it's important to see what's the relationship between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INJ_SEVNAME\n",
       "Fatal Injury (K)    241\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['DOA'] >= 8].groupby('INJ_SEVNAME').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when a person dies, he will always be tagged as having a fatal injury. </br>\n",
    "Does that imply all fatal injuries are tagged as death?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOANAME\n",
       "Died En Route       241\n",
       "Died at Scene     20121\n",
       "Not Applicable    15925\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['INJ_SEV'] == 4].groupby('DOANAME').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem to be the case.</br>\n",
    "So we'll tag an accident result as a fatal injury only when the person did not die.\n",
    "\n",
    "We'll create a new `accident_result` field that will be our prediction target and will be a combination of both `INJ_SEV` AND `DOA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_accident_result_columns(df, accident_result_column, accident_result_name_column):\n",
    "    \"\"\"\n",
    "    Creates a df of accident results for the inputted df\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the raw data\n",
    "    accident_result_column (str): The name of the new accident result column\n",
    "    accident_result_name_column (str): The name of the new accident result label column \n",
    "    \n",
    "    Returns:\n",
    "    result_df (pd.DataFrame): A df containing the actual accident result \n",
    "    per the the input df's rows\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a copy df containing only the columns necessary for inferring the accident result\n",
    "    result_df = df.filter(regex=r'^(INJ_SEV|DOA)', axis=1).copy()\n",
    "    \n",
    "    # Initialize accident_result_column and accident_result_name_column based on 'INJ_SEV' and 'INJ_SEVNAME'\n",
    "    result_df[accident_result_column] = result_df['INJ_SEV']\n",
    "    result_df[accident_result_name_column] = result_df['INJ_SEVNAME']\n",
    "    \n",
    "    # If the person died, update accident_result_column and accident_result_name_column as the corrsponding death type\n",
    "    mask_doa = result_df['DOA'] > 0\n",
    "    result_df.loc[mask_doa, accident_result_column] = result_df.loc[mask_doa, 'DOA']\n",
    "    result_df.loc[mask_doa, accident_result_name_column] = result_df.loc[mask_doa, 'DOANAME']\n",
    "    \n",
    "    # If the result was a fatal injury without death, \n",
    "    # we'll tag an accident result as a fatal injury only when the person did not die.\n",
    "    mask_fatal = result_df[accident_result_column] == 4\n",
    "    result_df.loc[mask_fatal, accident_result_name_column] = 'Fatal Injury without Death'\n",
    "    \n",
    "    # Clean accident_result_name_column labels\n",
    "    result_df[accident_result_name_column] = result_df[accident_result_name_column].str.extract(r'^(?P<accident_result_name>[^()]+)')\n",
    "    result_df[accident_result_name_column] = result_df[accident_result_name_column].str.strip()\n",
    "\n",
    "    result_df = result_df[[accident_result_column, accident_result_name_column]]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def make_id_label_dict(df, id_column, label_column):\n",
    "    \"\"\"\n",
    "    Creates a dictionary from a pandas DataFrame with IDs as keys and labels as values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing ID and label columns.\n",
    "    id_column (str): The name of the column containing the IDs.\n",
    "    label_column (str): The name of the column containing the labels.\n",
    "    \n",
    "    Returns:\n",
    "    id_label_dict (dict): A dictionary with IDs as keys and labels as values.\n",
    "    \"\"\"\n",
    "    # Drop duplicates to ensure unique ID-label pairs\n",
    "    unique_pairs = df[[id_column, label_column]].drop_duplicates()\n",
    "    \n",
    "    # Convert the unique pairs to a dictionary\n",
    "    id_label_dict = dict(zip(unique_pairs[id_column], unique_pairs[label_column]))\n",
    "    \n",
    "    return id_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_datasets(df):\n",
    "    \"\"\"\n",
    "    Creates a df containing only rows our model can train on,\n",
    "    a corresponding target Series for supervised learning,\n",
    "    And a label dictionary for inferring the target series values\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the raw data\n",
    "    including the columns necessary for creating the target column\n",
    "    \n",
    "    Returns:\n",
    "    train_df (pd.DataFrame): A df containing only rows our model can train on\n",
    "    target_df (pd.DataFrame): A pandas Series containing the target the model will learn on\n",
    "    accident_result_names (dict): A dictionary containing target series textual labels\n",
    "    \"\"\"\n",
    "    # As mentioned above, we'll train our model only on cases where \n",
    "    # the outcome of the person was known\n",
    "    training_mask = (df['INJ_SEV'] <= 4) & (df['DOA'] != 9)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    train_df = df[training_mask].copy()\n",
    "\n",
    "    # Names of the target column and its' corresponding textual label\n",
    "    accident_result_column = 'accident_result'\n",
    "    accident_result_name_column = accident_result_column + '_name'\n",
    "\n",
    "    # Creating a df containing only the target column and its' label\n",
    "    result_df = add_accident_result_columns(train_df, accident_result_column, accident_result_name_column)\n",
    "\n",
    "    # We'll keep a dict mapping the target ordinal value to its' label\n",
    "    accident_result_names = make_id_label_dict(result_df, accident_result_column, accident_result_name_column)\n",
    "\n",
    "    # Since we have the label dict there's no need to return the label column\n",
    "    target_df = result_df[accident_result_column]\n",
    "\n",
    "    return train_df, target_df, accident_result_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the labels\n",
    "We'll drop the name label columns for now, since they're only really useful when we'll want to present our prediction in a user friendly way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_LABEL_COLUMNS = ['STATENAME', 'INJ_SEV', 'INJ_SEVNAME',  'DOA', 'DOANAME']\n",
    "COLUMNS_FOR_MODEL = [column for column in INVESTIGATED_COLUMNS if column not in CATEGORY_LABEL_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(CATEGORY_LABEL_COLUMNS, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 81521 entries, 0 to 82842\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ST_CASE      81521 non-null  int64 \n",
      " 1   STATE        81521 non-null  int64 \n",
      " 2   STATENAME    81521 non-null  object\n",
      " 3   VEH_NO       81521 non-null  int64 \n",
      " 4   VE_FORMS     81521 non-null  int64 \n",
      " 5   PER_NO       81521 non-null  int64 \n",
      " 6   COUNTY       81521 non-null  int64 \n",
      " 7   DAY          81521 non-null  int64 \n",
      " 8   MONTH        81521 non-null  int64 \n",
      " 9   HOUR         81521 non-null  int64 \n",
      " 10  AGE          81521 non-null  int64 \n",
      " 11  SEX          81521 non-null  int64 \n",
      " 12  INJ_SEV      81521 non-null  int64 \n",
      " 13  INJ_SEVNAME  81521 non-null  object\n",
      " 14  DOA          81521 non-null  int64 \n",
      " 15  DOANAME      81521 non-null  object\n",
      " 16  SEAT_POS     81521 non-null  int64 \n",
      " 17  REST_USE     81521 non-null  int64 \n",
      "dtypes: int64(15), object(3)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST_CASE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>VEH_NO</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>PER_NO</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>DAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>INJ_SEV</th>\n",
       "      <th>DOA</th>\n",
       "      <th>SEAT_POS</th>\n",
       "      <th>REST_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>270314.092025</td>\n",
       "      <td>26.953178</td>\n",
       "      <td>1.368850</td>\n",
       "      <td>1.911802</td>\n",
       "      <td>1.514579</td>\n",
       "      <td>92.566688</td>\n",
       "      <td>15.694766</td>\n",
       "      <td>6.709523</td>\n",
       "      <td>13.556397</td>\n",
       "      <td>53.061407</td>\n",
       "      <td>1.407110</td>\n",
       "      <td>2.394181</td>\n",
       "      <td>1.751389</td>\n",
       "      <td>13.000454</td>\n",
       "      <td>23.831148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164632.985687</td>\n",
       "      <td>16.475802</td>\n",
       "      <td>1.273783</td>\n",
       "      <td>1.950195</td>\n",
       "      <td>1.379468</td>\n",
       "      <td>96.959194</td>\n",
       "      <td>8.829369</td>\n",
       "      <td>3.373724</td>\n",
       "      <td>8.582249</td>\n",
       "      <td>110.555037</td>\n",
       "      <td>0.859091</td>\n",
       "      <td>1.690066</td>\n",
       "      <td>3.035807</td>\n",
       "      <td>11.211551</td>\n",
       "      <td>34.128658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10001.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121609.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>260609.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>420469.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>560121.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>997.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ST_CASE         STATE        VEH_NO      VE_FORMS        PER_NO  \\\n",
       "count   81521.000000  81521.000000  81521.000000  81521.000000  81521.000000   \n",
       "mean   270314.092025     26.953178      1.368850      1.911802      1.514579   \n",
       "std    164632.985687     16.475802      1.273783      1.950195      1.379468   \n",
       "min     10001.000000      1.000000      0.000000      1.000000      1.000000   \n",
       "25%    121609.000000     12.000000      1.000000      1.000000      1.000000   \n",
       "50%    260609.000000     26.000000      1.000000      2.000000      1.000000   \n",
       "75%    420469.000000     42.000000      2.000000      2.000000      2.000000   \n",
       "max    560121.000000     56.000000     58.000000     59.000000     57.000000   \n",
       "\n",
       "             COUNTY           DAY         MONTH          HOUR           AGE  \\\n",
       "count  81521.000000  81521.000000  81521.000000  81521.000000  81521.000000   \n",
       "mean      92.566688     15.694766      6.709523     13.556397     53.061407   \n",
       "std       96.959194      8.829369      3.373724      8.582249    110.555037   \n",
       "min        0.000000      1.000000      1.000000      0.000000      0.000000   \n",
       "25%       31.000000      8.000000      4.000000      8.000000     24.000000   \n",
       "50%       71.000000     16.000000      7.000000     14.000000     38.000000   \n",
       "75%      115.000000     23.000000     10.000000     19.000000     57.000000   \n",
       "max      997.000000     31.000000     12.000000     99.000000    999.000000   \n",
       "\n",
       "                SEX       INJ_SEV           DOA      SEAT_POS      REST_USE  \n",
       "count  81521.000000  81521.000000  81521.000000  81521.000000  81521.000000  \n",
       "mean       1.407110      2.394181      1.751389     13.000454     23.831148  \n",
       "std        0.859091      1.690066      3.035807     11.211551     34.128658  \n",
       "min        1.000000      0.000000      0.000000      0.000000      1.000000  \n",
       "25%        1.000000      0.000000      0.000000     11.000000      3.000000  \n",
       "50%        1.000000      3.000000      0.000000     11.000000      3.000000  \n",
       "75%        2.000000      4.000000      0.000000     13.000000     20.000000  \n",
       "max        9.000000      4.000000      8.000000     99.000000     99.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1711, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['VEH_NO'] > 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4547, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['VE_FORMS'] > 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're actually thinking of the number of cars\n",
    "# as more of an ordinal variable rather than a numerical one\n",
    "# Meaning accidents with 4 or more cars is a \"category 4\" accident\n",
    "df.loc[df['VE_FORMS'] <= 3, 'VE_FORMS'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a reusable preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarsPreModelProcessor(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Keep only the specified columns\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Here we're actually thinking of the number of cars\n",
    "        # as more of an ordinal variable rather than a numerical one\n",
    "        # Meaning accidents with 4 or more cars is a \"category 4\" accident\n",
    "        X_transformed = X_transformed.apply(lambda value: value if value <= 3 else 4)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_columns = ['VE_FORMS']\n",
    "columns_without_processing = [col for col in COLUMNS_FOR_MODEL if col not in processed_columns]\n",
    "\n",
    "columns_without_processing_definition = gen_features(\n",
    "    columns = columns_without_processing,\n",
    "    classes = [None]\n",
    ")\n",
    "\n",
    "columns_preprocessing_definition = gen_features(\n",
    "    columns = ['VE_FORMS'],\n",
    "     classes= [FarsPreModelProcessor]\n",
    ")\n",
    "\n",
    "dataframe_preprocessing_definition = columns_preprocessing_definition + columns_without_processing_definition\n",
    "\n",
    "data_preparation_mapper = DataFrameMapper(\n",
    "    dataframe_preprocessing_definition,\n",
    "    input_df=True,\n",
    "    df_out=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running models\n",
    "For starters we'll try to create models that learn on the data as is.</br>\n",
    "This makes sense because not having specific information about a person in a motor accident could be valuable information by itself in predicting the person's injury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric we'll use for our model will be a weighted average of recall per class.\n",
    "\n",
    "The more severe an injury gets, the more important it is to decrease the amount of False Negative predictions of it, since the price of an error becomes more severe. \n",
    "Therefore, it makes sense to calculate the recall score of each injury class separately and then calculate an overall weighted average that gives higher importance to more severe injuries.\n",
    "\n",
    "For the sake of this exercise, we'll focus for now only on this metric as the only maximising metric and not take into account other satisfising factors like the predition speed of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_recall_score(true_labels: pd.Series, predicted_labels: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the weighted average recall score of each possible value of the true labels,\n",
    "    where the weight of each group is the value of the true label.\n",
    "\n",
    "    Args:\n",
    "    - true_labels (pd.Series): Series of true labels.\n",
    "    - predicted_labels (pd.Series): Series of predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Weighted average recall score.\n",
    "    \"\"\"\n",
    "    # Ensure the input series have the same length\n",
    "    assert len(true_labels) == len(predicted_labels), \"True and predicted labels must have the same length.\"\n",
    "    \n",
    "    # Get unique values in true labels\n",
    "    unique_labels = true_labels.unique()\n",
    "    \n",
    "    # Initialize variables to calculate weighted recall\n",
    "    total_weight = 0\n",
    "    weighted_recall_sum = 0\n",
    "    \n",
    "    # Calculate recall for each unique label and compute weighted sum\n",
    "    for label in unique_labels:\n",
    "        # Create boolean masks for the current label\n",
    "        true_mask = (true_labels == label)\n",
    "        evaluated_true_labels = true_labels[true_mask]\n",
    "        evaluated_pred_labels = predicted_labels[true_mask]\n",
    "        \n",
    "        # Calculate recall for the current label\n",
    "        recall = recall_score(evaluated_true_labels, evaluated_pred_labels, average=\"micro\")\n",
    "        \n",
    "        # Calculate the weight for the current label\n",
    "        weight = int(label)\n",
    "        \n",
    "        # Accumulate weighted recall and total weight\n",
    "        weighted_recall_sum += recall * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    # Calculate weighted average recall\n",
    "    weighted_avg_recall = weighted_recall_sum / total_weight if total_weight != 0 else 0\n",
    "    \n",
    "    return weighted_avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying out some simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to log the model and metrics to mlflow\n",
    "def log_model_with_mlflow(model_name, model, model_params,preprocess_params, artifacts, X_train, X_val, y_train, y_val):\n",
    "    with mlflow.start_run():\n",
    "        # Log preprocessing parameters\n",
    "        # for param, value in preprocess_params.items():\n",
    "        mlflow.log_params(preprocess_params)\n",
    "        \n",
    "        # Log model parameters\n",
    "        # for param, value in model_params.items():\n",
    "        mlflow.log_params(model_params)\n",
    "\n",
    "        # Set the parameters\n",
    "        model.set_params(**model_params)\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and calculate the wighted recall score for the training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_weighted_recall = weighted_recall_score(y_train, y_train_pred)\n",
    "        mlflow.log_metric('train_weighted_recall', train_weighted_recall)\n",
    "\n",
    "        # Predict and calculate the wighted recall score for the validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_weighted_recall = weighted_recall_score(y_val, y_val_pred)\n",
    "        mlflow.log_metric('val_weighted_recall', val_weighted_recall)\n",
    "        \n",
    "        # Log the model\n",
    "        if model_name == \"XGBClassifier\":\n",
    "            mlflow.xgboost.log_model(model, model_name)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "        # Log the artifacts\n",
    "        for artifact in artifacts:\n",
    "            if artifact['type'] == 'dict':\n",
    "                mlflow.log_dict(dictionary = artifact['object'], artifact_file = artifact['file_name'])\n",
    "            elif artifact['type'] == 'file':\n",
    "                mlflow.log_artifact(local_path = artifact['local_path'], artifact_path = artifact['artifact_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and parameter grids to be tested\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), [\n",
    "        {'n_estimators': 100, 'max_depth': 10},\n",
    "        {'n_estimators': 200, 'max_depth': 20},\n",
    "    ]),\n",
    "    # # 'SVC': (SVC(probability=True), [\n",
    "    # #     {'C': 1, 'kernel': 'linear'},\n",
    "    # #     {'C': 1, 'kernel': 'rbf'},\n",
    "    # # ]),\n",
    "    # # 'LogisticRegression': (LogisticRegression(), [\n",
    "    # #     {'penalty': 'l2', 'C': 1},\n",
    "    # #     {'penalty': 'l2', 'C': 0.1},\n",
    "    # # ]),\n",
    "    'GradientBoosting': (GradientBoostingClassifier(), [\n",
    "        {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "        {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    ]),\n",
    "    # 'XGBClassifier': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), [\n",
    "    #     {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    #     {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    # ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='gs://mlops_zoomcamp-mlflow-artifacts/artifacts/1', creation_time=1721555556438, experiment_id='1', last_update_time=1721555556438, lifecycle_stage='active', name='NHTSA FARS Injury prediction', tags={}>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"NHTSA FARS Injury prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1922/3712104930.py:1: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv('./data/FARS2021NationalCSV/person.csv', encoding='Windows-1252')\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv('./data/FARS2021NationalCSV/person.csv', encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the target column\n",
    "train_df, train_target_df, train_label_for_target = prep_training_datasets(df)\n",
    "val_df, val_target_df, val_label_for_target = prep_training_datasets(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_label_for_target == val_label_for_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_params = {\n",
    "    'VE_FORMS_outlier_strategy': '>=4 becomes category 4',\n",
    "}\n",
    "\n",
    "artifacts = [\n",
    "    { \n",
    "        'type': 'dict',\n",
    "        'object': train_label_for_target,\n",
    "        'file_name': 'NHTSA_FARS_labels_for_target.json',\n",
    "    },\n",
    "    # { \n",
    "    #     'type': 'file',\n",
    "    #     'local_path': label_for_target_file,\n",
    "    #     'artifact_path': 'label_for_target',\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function tosequence is deprecated; tosequence was deprecated in 1.5 and will be removed in 1.7\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5 6], got [0 1 2 3 4 7 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m model_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     14\u001b[0m     pipeline_model_params[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_pipe_step_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 16\u001b[0m \u001b[43mlog_model_with_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpipeline_model_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpreprocessing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                      \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_target_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                      \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_target_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 16\u001b[0m, in \u001b[0;36mlog_model_with_mlflow\u001b[0;34m(model_name, model, model_params, preprocess_params, artifacts, X_train, X_val, y_train, y_val)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Predict and calculate the wighted recall score for the training set\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/xgboost/sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1490\u001b[0m ):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1494\u001b[0m     )\n\u001b[1;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5 6], got [0 1 2 3 4 7 8]"
     ]
    }
   ],
   "source": [
    "model_pipe_step_name = 'classifier'\n",
    "\n",
    "# Train and log each model using a pipeline\n",
    "for model_name, (model, param_list) in models.items():\n",
    "    for model_params in param_list:\n",
    "        pipeline = Pipeline([\n",
    "            # ('scaler', StandardScaler()),\n",
    "            ('general_preprocessor', data_preparation_mapper),\n",
    "            (model_pipe_step_name, model)\n",
    "        ])\n",
    "\n",
    "        pipeline_model_params = {}\n",
    "        for param, value in model_params.items():\n",
    "            pipeline_model_params[f'{model_pipe_step_name}__{param}'] = value\n",
    "\n",
    "        log_model_with_mlflow(model_name = model_name,\n",
    "                              model = pipeline,\n",
    "                              model_params = pipeline_model_params,\n",
    "                              preprocess_params = preprocessing_params,\n",
    "                              artifacts = artifacts,\n",
    "                              X_train = train_df,\n",
    "                              X_val = val_df,\n",
    "                              y_train = train_target_df,\n",
    "                              y_val = val_target_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NHTSA-FARS-MLOps-Project-r_D6DKIt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
