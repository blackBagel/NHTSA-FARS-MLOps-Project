{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import recall_score\n",
    "import mlflow\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn_pandas import DataFrameMapper, gen_features\n",
    "from sklearn.base import TransformerMixin\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What will happen to you if you were in a car accident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "\n",
    "# with open('./data/accident.CSV', 'rb') as filedata:\n",
    "#     result = chardet.detect(filedata.read(10000000))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './data/datasets/'\n",
    "data_file_name = 'person.csv'\n",
    "\n",
    "TRAIN_PATH = f'{DATASET_DIR}/2019/{data_file_name}'\n",
    "VALIDATION_PATH = f'{DATASET_DIR}/2021/{data_file_name}'\n",
    "TEST_PATH = f'{DATASET_DIR}/2022/{data_file_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1813/1528518912.py:1: DtypeWarning: Columns (15,106,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(TRAIN_PATH, encoding='Windows-1252')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATENAME</th>\n",
       "      <th>ST_CASE</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>VEH_NO</th>\n",
       "      <th>PER_NO</th>\n",
       "      <th>STR_VEH</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAYNAME</th>\n",
       "      <th>...</th>\n",
       "      <th>WORK_INJ</th>\n",
       "      <th>WORK_INJNAME</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>HISPANICNAME</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>LOCATIONNAME</th>\n",
       "      <th>HELM_USE</th>\n",
       "      <th>HELM_USENAME</th>\n",
       "      <th>HELM_MIS</th>\n",
       "      <th>HELM_MISNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Applicable (not a fatality)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not A Fatality (not Applicable)</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Applicable (not a fatality)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not A Fatality (not Applicable)</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10002</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>10002</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Applicable (not a fatality)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not A Fatality (not Applicable)</td>\n",
       "      <td>0</td>\n",
       "      <td>Occupant of a Motor Vehicle</td>\n",
       "      <td>20</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>7</td>\n",
       "      <td>None Used/Not Applicable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE STATENAME  ST_CASE  VE_FORMS  VEH_NO  PER_NO  STR_VEH  COUNTY  DAY  \\\n",
       "0      1   Alabama    10001         2       1       1        0      81    7   \n",
       "1      1   Alabama    10001         2       1       2        0      81    7   \n",
       "2      1   Alabama    10001         2       2       1        0      81    7   \n",
       "3      1   Alabama    10002         2       1       1        0      55   23   \n",
       "4      1   Alabama    10002         2       2       1        0      55   23   \n",
       "\n",
       "   DAYNAME  ...  WORK_INJ                     WORK_INJNAME  HISPANIC  \\\n",
       "0        7  ...         8  Not Applicable (not a fatality)         0   \n",
       "1        7  ...         0                               No         7   \n",
       "2        7  ...         8  Not Applicable (not a fatality)         0   \n",
       "3       23  ...         0                               No         7   \n",
       "4       23  ...         8  Not Applicable (not a fatality)         0   \n",
       "\n",
       "                      HISPANICNAME  LOCATION                 LOCATIONNAME  \\\n",
       "0  Not A Fatality (not Applicable)         0  Occupant of a Motor Vehicle   \n",
       "1                     Non-Hispanic         0  Occupant of a Motor Vehicle   \n",
       "2  Not A Fatality (not Applicable)         0  Occupant of a Motor Vehicle   \n",
       "3                     Non-Hispanic         0  Occupant of a Motor Vehicle   \n",
       "4  Not A Fatality (not Applicable)         0  Occupant of a Motor Vehicle   \n",
       "\n",
       "   HELM_USE    HELM_USENAME  HELM_MIS              HELM_MISNAME  \n",
       "0        20  Not Applicable         7  None Used/Not Applicable  \n",
       "1        20  Not Applicable         7  None Used/Not Applicable  \n",
       "2        20  Not Applicable         7  None Used/Not Applicable  \n",
       "3        20  Not Applicable         7  None Used/Not Applicable  \n",
       "4        20  Not Applicable         7  None Used/Not Applicable  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(TRAIN_PATH, encoding='Windows-1252')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['STATE', 'STATENAME', 'ST_CASE', 'VE_FORMS', 'VEH_NO', 'PER_NO',\n",
       "       'STR_VEH', 'COUNTY', 'DAY', 'DAYNAME', 'MONTH', 'MONTHNAME',\n",
       "       'HOUR', 'HOURNAME', 'MINUTE', 'MINUTENAME', 'RUR_URB',\n",
       "       'RUR_URBNAME', 'FUNC_SYS', 'FUNC_SYSNAME', 'HARM_EV',\n",
       "       'HARM_EVNAME', 'MAN_COLL', 'MAN_COLLNAME', 'SCH_BUS',\n",
       "       'SCH_BUSNAME', 'MAKE', 'MAKENAME', 'MAK_MOD', 'BODY_TYP',\n",
       "       'BODY_TYPNAME', 'MOD_YEAR', 'MOD_YEARNAME', 'TOW_VEH',\n",
       "       'TOW_VEHNAME', 'SPEC_USE', 'SPEC_USENAME', 'EMER_USE',\n",
       "       'EMER_USENAME', 'ROLLOVER', 'ROLLOVERNAME', 'IMPACT1',\n",
       "       'IMPACT1NAME', 'FIRE_EXP', 'FIRE_EXPNAME', 'AGE', 'AGENAME', 'SEX',\n",
       "       'SEXNAME', 'PER_TYP', 'PER_TYPNAME', 'INJ_SEV', 'INJ_SEVNAME',\n",
       "       'SEAT_POS', 'SEAT_POSNAME', 'REST_USE', 'REST_USENAME', 'REST_MIS',\n",
       "       'REST_MISNAME', 'AIR_BAG', 'AIR_BAGNAME', 'EJECTION',\n",
       "       'EJECTIONNAME', 'EJ_PATH', 'EJ_PATHNAME', 'EXTRICAT',\n",
       "       'EXTRICATNAME', 'DRINKING', 'DRINKINGNAME', 'ALC_DET',\n",
       "       'ALC_DETNAME', 'ALC_STATUS', 'ALC_STATUSNAME', 'ATST_TYP',\n",
       "       'ATST_TYPNAME', 'ALC_RES', 'ALC_RESNAME', 'DRUGS', 'DRUGSNAME',\n",
       "       'DRUG_DET', 'DRUG_DETNAME', 'DSTATUS', 'DSTATUSNAME', 'HOSPITAL',\n",
       "       'HOSPITALNAME', 'DOA', 'DOANAME', 'DEATH_DA', 'DEATH_DANAME',\n",
       "       'DEATH_MO', 'DEATH_MONAME', 'DEATH_YR', 'DEATH_YRNAME', 'DEATH_HR',\n",
       "       'DEATH_HRNAME', 'DEATH_MN', 'DEATH_MNNAME', 'DEATH_TM',\n",
       "       'DEATH_TMNAME', 'LAG_HRS', 'LAG_HRSNAME', 'LAG_MINS',\n",
       "       'LAG_MINSNAME', 'P_SF1', 'P_SF1NAME', 'P_SF2', 'P_SF2NAME',\n",
       "       'P_SF3', 'P_SF3NAME', 'WORK_INJ', 'WORK_INJNAME', 'HISPANIC',\n",
       "       'HISPANICNAME', 'LOCATION', 'LOCATIONNAME', 'HELM_USE',\n",
       "       'HELM_USENAME', 'HELM_MIS', 'HELM_MISNAME'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVESTIGATED_COLUMNS = [\n",
    "    'ST_CASE', \n",
    "    'STATE',\n",
    "    'STATENAME',\n",
    "    'VEH_NO',\n",
    "    'VE_FORMS',\n",
    "    'PER_NO',\n",
    "    'COUNTY',\n",
    "    'DAY',\n",
    "    'MONTH',\n",
    "    'HOUR',\n",
    "    'AGE',\n",
    "    'SEX',\n",
    "    'INJ_SEV',\n",
    "    'INJ_SEVNAME',\n",
    "    'DOA',\n",
    "    'DOANAME',\n",
    "    'SEAT_POS',\n",
    "    'REST_USE',\n",
    "]\n",
    "TARGET = 'INJ_SEV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "df = df[INVESTIGATED_COLUMNS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping irrelevant injury categories\n",
    "\n",
    "The dataset user manual states these are the possible values for the injury severity field (`INJ_SEV`):\n",
    "\n",
    "- 0 - No Apparent Injury\n",
    "- 1 - Possible Injury\n",
    "- 2 - Suspected Minor Injury\n",
    "- 3 - Suspected Serious Injury\n",
    "- 4 - Fatal Injury \n",
    "- 5 - Injured, Severity Unknown\n",
    "- 6 - Died Prior to Crash\n",
    "- 9 - Unknown/Not Reported \n",
    "\n",
    "Since we want to teach our model to predict a specific injury severity, we'll only use categories 0-4\n",
    "\n",
    "In addition, we'll consider death as another type of injury. </br>\n",
    "The user manual describes the column detailing death (`DOA`) as such:\n",
    "\n",
    "- 0 Not Applicable \n",
    "- 7 Died at Scene\n",
    "- 8 Died En Route (to a hospital)\n",
    "- 9 Unknown\n",
    "\n",
    "Again, we'll ignore cases where death is unknown and focus on categories 0,7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['INJ_SEV'] <= 4) & (df['DOA'] != 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to combine the injury severity and death outcome results to one field, it's important to see what's the relationship between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INJ_SEVNAME\n",
       "Fatal Injury (K)    241\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['DOA'] >= 8].groupby('INJ_SEVNAME').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when a person dies, he will always be tagged as having a fatal injury. </br>\n",
    "Does that imply all fatal injuries are tagged as death?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOANAME\n",
       "Died En Route       241\n",
       "Died at Scene     20121\n",
       "Not Applicable    15925\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['INJ_SEV'] == 4].groupby('DOANAME').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem to be the case.</br>\n",
    "So we'll tag an accident result as a fatal injury only when the person did not die.\n",
    "\n",
    "We'll create a new `accident_result` field that will be our prediction target and will be a combination of both `INJ_SEV` AND `DOA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_accident_result_columns(df, accident_result_column, accident_result_name_column):\n",
    "    \"\"\"\n",
    "    Creates a df of accident results for the inputted df\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the raw data\n",
    "    accident_result_column (str): The name of the new accident result column\n",
    "    accident_result_name_column (str): The name of the new accident result label column \n",
    "    \n",
    "    Returns:\n",
    "    result_df (pd.DataFrame): A df containing the actual accident result \n",
    "    per the the input df's rows\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a copy df containing only the columns necessary for inferring the accident result\n",
    "    result_df = df.filter(regex=r'^(INJ_SEV|DOA)', axis=1).copy()\n",
    "    \n",
    "    # Initialize accident_result_column and accident_result_name_column based on 'INJ_SEV' and 'INJ_SEVNAME'\n",
    "    result_df[accident_result_column] = result_df['INJ_SEV']\n",
    "    result_df[accident_result_name_column] = result_df['INJ_SEVNAME']\n",
    "    \n",
    "    # If the person died, update accident_result_column and accident_result_name_column as the corrsponding death type\n",
    "    mask_doa = result_df['DOA'] > 0\n",
    "    result_df.loc[mask_doa, accident_result_column] = result_df.loc[mask_doa, 'DOA']\n",
    "    result_df.loc[mask_doa, accident_result_name_column] = result_df.loc[mask_doa, 'DOANAME']\n",
    "    \n",
    "    # If the result was a fatal injury without death, \n",
    "    # we'll tag an accident result as a fatal injury only when the person did not die.\n",
    "    mask_fatal = result_df[accident_result_column] == 4\n",
    "    result_df.loc[mask_fatal, accident_result_name_column] = 'Fatal Injury without Death'\n",
    "    \n",
    "    # Clean accident_result_name_column labels\n",
    "    result_df[accident_result_name_column] = result_df[accident_result_name_column].str.extract(r'^(?P<accident_result_name>[^()]+)')\n",
    "    result_df[accident_result_name_column] = result_df[accident_result_name_column].str.strip()\n",
    "\n",
    "    result_df = result_df[[accident_result_column, accident_result_name_column]]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def make_id_label_dict(df, id_column, label_column):\n",
    "    \"\"\"\n",
    "    Creates a dictionary from a pandas DataFrame with IDs as keys and labels as values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing ID and label columns.\n",
    "    id_column (str): The name of the column containing the IDs.\n",
    "    label_column (str): The name of the column containing the labels.\n",
    "    \n",
    "    Returns:\n",
    "    id_label_dict (dict): A dictionary with IDs as keys and labels as values.\n",
    "    \"\"\"\n",
    "    # Drop duplicates to ensure unique ID-label pairs\n",
    "    unique_pairs = df[[id_column, label_column]].drop_duplicates()\n",
    "    \n",
    "    # Convert the unique pairs to a dictionary\n",
    "    id_label_dict = dict(zip(unique_pairs[id_column], unique_pairs[label_column]))\n",
    "    \n",
    "    return id_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_datasets(df):\n",
    "    \"\"\"\n",
    "    Creates a df containing only rows our model can train on,\n",
    "    a corresponding target Series for supervised learning,\n",
    "    And a label dictionary for inferring the target series values\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the raw data\n",
    "    including the columns necessary for creating the target column\n",
    "    \n",
    "    Returns:\n",
    "    train_df (pd.DataFrame): A df containing only rows our model can train on\n",
    "    target_df (pd.DataFrame): A pandas Series containing the target the model will learn on\n",
    "    accident_result_names (dict): A dictionary containing target series textual labels\n",
    "    \"\"\"\n",
    "    # As mentioned above, we'll train our model only on cases where \n",
    "    # the outcome of the person was known\n",
    "    training_mask = (df['INJ_SEV'] <= 4) & (df['DOA'] != 9)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    train_df = df[training_mask].copy()\n",
    "\n",
    "    # Names of the target column and its' corresponding textual label\n",
    "    accident_result_column = 'accident_result'\n",
    "    accident_result_name_column = accident_result_column + '_name'\n",
    "\n",
    "    # Creating a df containing only the target column and its' label\n",
    "    result_df = add_accident_result_columns(train_df, accident_result_column, accident_result_name_column)\n",
    "\n",
    "    # We'll keep a dict mapping the target ordinal value to its' label\n",
    "    accident_result_names = make_id_label_dict(result_df, accident_result_column, accident_result_name_column)\n",
    "\n",
    "    # Since we have the label dict there's no need to return the label column\n",
    "    target_df = result_df[accident_result_column]\n",
    "\n",
    "    return train_df, target_df, accident_result_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the labels\n",
    "We'll drop the name label columns for now, since they're only really useful when we'll want to present our prediction in a user friendly way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_LABEL_COLUMNS = ['STATENAME', 'INJ_SEV', 'INJ_SEVNAME',  'DOA', 'DOANAME']\n",
    "COLUMNS_FOR_MODEL = [column for column in INVESTIGATED_COLUMNS if column not in CATEGORY_LABEL_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(CATEGORY_LABEL_COLUMNS, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 81521 entries, 0 to 82842\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ST_CASE      81521 non-null  int64 \n",
      " 1   STATE        81521 non-null  int64 \n",
      " 2   STATENAME    81521 non-null  object\n",
      " 3   VEH_NO       81521 non-null  int64 \n",
      " 4   VE_FORMS     81521 non-null  int64 \n",
      " 5   PER_NO       81521 non-null  int64 \n",
      " 6   COUNTY       81521 non-null  int64 \n",
      " 7   DAY          81521 non-null  int64 \n",
      " 8   MONTH        81521 non-null  int64 \n",
      " 9   HOUR         81521 non-null  int64 \n",
      " 10  AGE          81521 non-null  int64 \n",
      " 11  SEX          81521 non-null  int64 \n",
      " 12  INJ_SEV      81521 non-null  int64 \n",
      " 13  INJ_SEVNAME  81521 non-null  object\n",
      " 14  DOA          81521 non-null  int64 \n",
      " 15  DOANAME      81521 non-null  object\n",
      " 16  SEAT_POS     81521 non-null  int64 \n",
      " 17  REST_USE     81521 non-null  int64 \n",
      "dtypes: int64(15), object(3)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST_CASE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>VEH_NO</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>PER_NO</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>DAY</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>INJ_SEV</th>\n",
       "      <th>DOA</th>\n",
       "      <th>SEAT_POS</th>\n",
       "      <th>REST_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "      <td>81521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>270314.092025</td>\n",
       "      <td>26.953178</td>\n",
       "      <td>1.368850</td>\n",
       "      <td>1.911802</td>\n",
       "      <td>1.514579</td>\n",
       "      <td>92.566688</td>\n",
       "      <td>15.694766</td>\n",
       "      <td>6.709523</td>\n",
       "      <td>13.556397</td>\n",
       "      <td>53.061407</td>\n",
       "      <td>1.407110</td>\n",
       "      <td>2.394181</td>\n",
       "      <td>1.751389</td>\n",
       "      <td>13.000454</td>\n",
       "      <td>23.831148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164632.985687</td>\n",
       "      <td>16.475802</td>\n",
       "      <td>1.273783</td>\n",
       "      <td>1.950195</td>\n",
       "      <td>1.379468</td>\n",
       "      <td>96.959194</td>\n",
       "      <td>8.829369</td>\n",
       "      <td>3.373724</td>\n",
       "      <td>8.582249</td>\n",
       "      <td>110.555037</td>\n",
       "      <td>0.859091</td>\n",
       "      <td>1.690066</td>\n",
       "      <td>3.035807</td>\n",
       "      <td>11.211551</td>\n",
       "      <td>34.128658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10001.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121609.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>260609.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>420469.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>560121.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>997.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ST_CASE         STATE        VEH_NO      VE_FORMS        PER_NO  \\\n",
       "count   81521.000000  81521.000000  81521.000000  81521.000000  81521.000000   \n",
       "mean   270314.092025     26.953178      1.368850      1.911802      1.514579   \n",
       "std    164632.985687     16.475802      1.273783      1.950195      1.379468   \n",
       "min     10001.000000      1.000000      0.000000      1.000000      1.000000   \n",
       "25%    121609.000000     12.000000      1.000000      1.000000      1.000000   \n",
       "50%    260609.000000     26.000000      1.000000      2.000000      1.000000   \n",
       "75%    420469.000000     42.000000      2.000000      2.000000      2.000000   \n",
       "max    560121.000000     56.000000     58.000000     59.000000     57.000000   \n",
       "\n",
       "             COUNTY           DAY         MONTH          HOUR           AGE  \\\n",
       "count  81521.000000  81521.000000  81521.000000  81521.000000  81521.000000   \n",
       "mean      92.566688     15.694766      6.709523     13.556397     53.061407   \n",
       "std       96.959194      8.829369      3.373724      8.582249    110.555037   \n",
       "min        0.000000      1.000000      1.000000      0.000000      0.000000   \n",
       "25%       31.000000      8.000000      4.000000      8.000000     24.000000   \n",
       "50%       71.000000     16.000000      7.000000     14.000000     38.000000   \n",
       "75%      115.000000     23.000000     10.000000     19.000000     57.000000   \n",
       "max      997.000000     31.000000     12.000000     99.000000    999.000000   \n",
       "\n",
       "                SEX       INJ_SEV           DOA      SEAT_POS      REST_USE  \n",
       "count  81521.000000  81521.000000  81521.000000  81521.000000  81521.000000  \n",
       "mean       1.407110      2.394181      1.751389     13.000454     23.831148  \n",
       "std        0.859091      1.690066      3.035807     11.211551     34.128658  \n",
       "min        1.000000      0.000000      0.000000      0.000000      1.000000  \n",
       "25%        1.000000      0.000000      0.000000     11.000000      3.000000  \n",
       "50%        1.000000      3.000000      0.000000     11.000000      3.000000  \n",
       "75%        2.000000      4.000000      0.000000     13.000000     20.000000  \n",
       "max        9.000000      4.000000      8.000000     99.000000     99.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1711, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['VEH_NO'] > 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4547, 18)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['VE_FORMS'] > 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're actually thinking of the number of cars\n",
    "# as more of an ordinal variable rather than a numerical one\n",
    "# Meaning accidents with 4 or more cars is a \"category 4\" accident\n",
    "df.loc[df['VE_FORMS'] <= 3, 'VE_FORMS'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a reusable preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarsPreModelProcessor(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Keep only the specified columns\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Here we're actually thinking of the number of cars\n",
    "        # as more of an ordinal variable rather than a numerical one\n",
    "        # Meaning accidents with 4 or more cars is a \"category 4\" accident\n",
    "        X_transformed = X_transformed.apply(lambda value: value if value <= 3 else 4)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_columns = ['VE_FORMS']\n",
    "columns_without_processing = [col for col in COLUMNS_FOR_MODEL if col not in processed_columns]\n",
    "\n",
    "columns_without_processing_definition = gen_features(\n",
    "    columns = columns_without_processing,\n",
    "    classes = [None]\n",
    ")\n",
    "\n",
    "columns_preprocessing_definition = gen_features(\n",
    "    columns = ['VE_FORMS'],\n",
    "     classes= [FarsPreModelProcessor]\n",
    ")\n",
    "\n",
    "dataframe_preprocessing_definition = columns_preprocessing_definition + columns_without_processing_definition\n",
    "\n",
    "data_preparation_mapper = DataFrameMapper(\n",
    "    dataframe_preprocessing_definition,\n",
    "    input_df=True,\n",
    "    df_out=True,\n",
    ")\n",
    "\n",
    "data_prep_steps = [('general_preprocessor', data_preparation_mapper)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running models\n",
    "For starters we'll try to create models that learn on the data as is.</br>\n",
    "This makes sense because not having specific information about a person in a motor accident could be valuable information by itself in predicting the person's injury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric we'll use for our model will be a weighted average of recall per class.\n",
    "\n",
    "The more severe an injury gets, the more important it is to decrease the amount of False Negative predictions of it, since the price of an error becomes more severe. \n",
    "Therefore, it makes sense to calculate the recall score of each injury class separately and then calculate an overall weighted average that gives higher importance to more severe injuries.\n",
    "\n",
    "For the sake of this exercise, we'll focus for now only on this metric as the only maximising metric and not take into account other satisfising factors like the predition speed of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_recall_score(true_labels: pd.Series, predicted_labels: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the weighted average recall score of each possible value of the true labels,\n",
    "    where the weight of each group is the value of the true label.\n",
    "\n",
    "    Args:\n",
    "    - true_labels (pd.Series): Series of true labels.\n",
    "    - predicted_labels (pd.Series): Series of predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Weighted average recall score.\n",
    "    \"\"\"\n",
    "    # Ensure the input series have the same length\n",
    "    assert len(true_labels) == len(predicted_labels), \"True and predicted labels must have the same length.\"\n",
    "    \n",
    "    # Get unique values in true labels\n",
    "    unique_labels = true_labels.unique()\n",
    "    \n",
    "    # Initialize variables to calculate weighted recall\n",
    "    total_weight = 0\n",
    "    weighted_recall_sum = 0\n",
    "    \n",
    "    # Calculate recall for each unique label and compute weighted sum\n",
    "    for label in unique_labels:\n",
    "        # Create boolean masks for the current label\n",
    "        true_mask = (true_labels == label)\n",
    "        evaluated_true_labels = true_labels[true_mask]\n",
    "        evaluated_pred_labels = predicted_labels[true_mask]\n",
    "        \n",
    "        # Calculate recall for the current label\n",
    "        recall = recall_score(evaluated_true_labels, evaluated_pred_labels, average=\"micro\")\n",
    "        \n",
    "        # Calculate the weight for the current label\n",
    "        weight = int(label)\n",
    "        \n",
    "        # Accumulate weighted recall and total weight\n",
    "        weighted_recall_sum += recall * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    # Calculate weighted average recall\n",
    "    weighted_avg_recall = weighted_recall_sum / total_weight if total_weight != 0 else 0\n",
    "    \n",
    "    return weighted_avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.Series([1, 1, 1, 8, 8])\n",
    "df2 = pd.Series([1, 1, 1, 8, 1])\n",
    "\n",
    "expected_result = (1*1 + 0.5*8)/(1+8)\n",
    "real_result = weighted_recall_score(df1, df2)\n",
    "\n",
    "assert expected_result == real_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying out some simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_model_params(model_pipe_step_name, models):\n",
    "    \"\"\"\n",
    "    Prefixes the model parameter keys with the given model pipeline step name.\n",
    "    This prefixing is necessary for the params to be added to the sklearn.pipeline model step\n",
    "\n",
    "    Args:\n",
    "    model_pipe_step_name (str): The model pipeline step name to prefix the parameter keys with.\n",
    "    models (dict): A dictionary containing model definitions and parameters.\n",
    "\n",
    "    Returns:\n",
    "    dict: A new dictionary with the prefixed parameter keys.\n",
    "    \"\"\"\n",
    "    prefixed_models = {}\n",
    "    \n",
    "    for model_name, (model, params_list) in models.items():\n",
    "        prefixed_params_list = []\n",
    "        for params in params_list:\n",
    "            prefixed_params = {f'{model_pipe_step_name}__{key}': value for key, value in params.items()}\n",
    "            prefixed_params_list.append(prefixed_params)\n",
    "        prefixed_models[model_name] = (model, prefixed_params_list)\n",
    "    \n",
    "    return prefixed_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and parameter grids to be tested\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), [\n",
    "        {'n_estimators': 100, 'max_depth': 10},\n",
    "        {'n_estimators': 200, 'max_depth': 20},\n",
    "    ]),\n",
    "    # # 'SVC': (SVC(probability=True), [\n",
    "    # #     {'C': 1, 'kernel': 'linear'},\n",
    "    # #     {'C': 1, 'kernel': 'rbf'},\n",
    "    # # ]),\n",
    "    # # 'LogisticRegression': (LogisticRegression(), [\n",
    "    # #     {'penalty': 'l2', 'C': 1},\n",
    "    # #     {'penalty': 'l2', 'C': 0.1},\n",
    "    # # ]),\n",
    "    'GradientBoosting': (GradientBoostingClassifier(), [\n",
    "        {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "        {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    ]),\n",
    "    # 'XGBClassifier': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), [\n",
    "    #     {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    #     {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    # ]),\n",
    "}\n",
    "\n",
    "model_pipe_step_name = 'classifier'\n",
    "models = prefix_model_params(model_pipe_step_name, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='gs://mlops_zoomcamp-mlflow-artifacts/artifacts/1', creation_time=1721555556438, experiment_id='1', last_update_time=1721555556438, lifecycle_stage='active', name='NHTSA FARS Injury prediction', tags={}>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_EXPERIMENT_NAME = \"NHTSA FARS Injury prediction\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1813/506465049.py:1: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(VALIDATION_PATH, encoding='Windows-1252')\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(VALIDATION_PATH, encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the target column\n",
    "train_df, train_target_df, train_label_for_target = prep_training_datasets(df)\n",
    "val_df, val_target_df, val_label_for_target = prep_training_datasets(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_label_for_target == val_label_for_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_params = {\n",
    "    'VE_FORMS_outlier_strategy': '>=4 becomes category 4',\n",
    "}\n",
    "\n",
    "artifacts = [\n",
    "    { \n",
    "        'type': 'dict',\n",
    "        'object': train_label_for_target,\n",
    "        'file_name': 'NHTSA_FARS_labels_for_target.json',\n",
    "    },\n",
    "    # { \n",
    "    #     'type': 'file',\n",
    "    #     'local_path': label_for_target_file,\n",
    "    #     'artifact_path': 'label_for_target',\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to log the model and metrics to mlflow\n",
    "def log_pipeline_with_mlflow(model_name, model, model_params, preprocess_params, artifacts, X_train, X_val, y_train, y_val, is_validation_set_test = False):\n",
    "    with mlflow.start_run():\n",
    "        # Log the model type as a tag for easy filtering\n",
    "        mlflow.set_tag(key = 'model_name', value = model_name)\n",
    "\n",
    "        # Log preprocessing parameters\n",
    "        mlflow.log_params(preprocess_params)\n",
    "        \n",
    "        # Log model parameters\n",
    "        mlflow.log_params(model_params)\n",
    "\n",
    "        # Set the parameters\n",
    "        model.set_params(**model_params)\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and calculate the wighted recall score for the training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_weighted_recall = weighted_recall_score(y_train, y_train_pred)\n",
    "        mlflow.log_metric('train_weighted_recall', train_weighted_recall)\n",
    "\n",
    "        # Predict and calculate the wighted recall score for the validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_weighted_recall = weighted_recall_score(y_val, y_val_pred)\n",
    "\n",
    "        if not is_validation_set_test:\n",
    "            mlflow.log_metric('val_weighted_recall', val_weighted_recall)\n",
    "        else:\n",
    "            mlflow.log_metric('test_weighted_recall', val_weighted_recall)\n",
    "        \n",
    "        # Log the model\n",
    "        if model_name == \"XGBClassifier\":\n",
    "            mlflow.xgboost.log_model(model, model_name)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "        # Log the artifacts\n",
    "        for artifact in artifacts:\n",
    "            if artifact['type'] == 'dict':\n",
    "                mlflow.log_dict(dictionary = artifact['object'], artifact_file = artifact['file_name'])\n",
    "            elif artifact['type'] == 'file':\n",
    "                mlflow.log_artifact(local_path = artifact['local_path'], artifact_path = artifact['artifact_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and log each model using a pipeline\n",
    "def train_and_log_pipelines(models, data_prep_steps, preprocessing_params, artifacts, X_train, X_val, y_train, y_val, is_validation_set_test = False):\n",
    "    for model_name, (model, model_params_combinations) in models.items():\n",
    "        for model_params_combination in model_params_combinations:\n",
    "            pipeline_steps = data_prep_steps.copy()\n",
    "            pipeline_steps.append((model_pipe_step_name, model))\n",
    "            pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "            log_pipeline_with_mlflow(model_name = model_name,\n",
    "                                    model = pipeline,\n",
    "                                    model_params = model_params_combination,\n",
    "                                    preprocess_params = preprocessing_params,\n",
    "                                    artifacts = artifacts,\n",
    "                                    X_train = X_train,\n",
    "                                    X_val = X_val,\n",
    "                                    y_train = y_train,\n",
    "                                    y_val = y_val,\n",
    "                                    is_validation_set_test=is_validation_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_log_pipelines(\n",
    "    models = models,\n",
    "    data_prep_steps = data_prep_steps,\n",
    "    preprocessing_params = preprocessing_params,\n",
    "    artifacts = artifacts,\n",
    "    X_train = train_df,\n",
    "    X_val = val_df,\n",
    "    y_train = train_target_df,\n",
    "    y_val = val_target_df,\n",
    "    is_validation_set_test = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productionizing the best model\n",
    "\n",
    "The results for our models are honestly pretty bad. </br>\n",
    "But since the purpose of our project is productionizing our model, this will suffice for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_by_regex(input_dict, pattern):\n",
    "    \"\"\"\n",
    "    Filters a dictionary by keys that match a given regex pattern.\n",
    "\n",
    "    Args:\n",
    "    input_dict (dict): The input dictionary to be filtered.\n",
    "    pattern (str): The regex pattern to match the keys.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two dictionaries, the first with key-value pairs where the keys match the regex pattern,\n",
    "           and the second with key-value pairs where the keys do not match the regex pattern.\n",
    "    \"\"\"\n",
    "    regex = re.compile(pattern)\n",
    "    matching_dict = {}\n",
    "    non_matching_dict = {}\n",
    "    \n",
    "    for key, value in input_dict.items():\n",
    "        if regex.match(key):\n",
    "            matching_dict[key] = value\n",
    "        else:\n",
    "            non_matching_dict[key] = value\n",
    "    \n",
    "    return matching_dict, non_matching_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1813/1638422707.py:175: DtypeWarning: Columns (15,106,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(TRAIN_PATH, encoding='Windows-1252')\n",
      "/tmp/ipykernel_1813/1638422707.py:176: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(VALIDATION_PATH, encoding='Windows-1252')\n",
      "/tmp/ipykernel_1813/1638422707.py:177: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(TEST_PATH, encoding='Windows-1252')\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function tosequence is deprecated; tosequence was deprecated in 1.5 and will be removed in 1.7\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function tosequence is deprecated; tosequence was deprecated in 1.5 and will be removed in 1.7\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function tosequence is deprecated; tosequence was deprecated in 1.5 and will be removed in 1.7\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/mlops_zoomcamp_gcp/.local/share/virtualenvs/NHTSA-FARS-MLOps-Project-r_D6DKIt/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from inspect import signature\n",
    "\n",
    "def get_best_n_runs(experiment_name, metric_name, n=3, is_higher_better = True):\n",
    "    \"\"\"\n",
    "    Retrieve the top n runs based on a specified metric.\n",
    "\n",
    "    Args:\n",
    "    experiment_name (str): The name of the MLflow experiment.\n",
    "    metric_name (str): The name of the metric to sort by.\n",
    "    n (int): The number of top runs to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    List of run info of the top n runs.\n",
    "    \"\"\"\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    sort_order = 'DESC' if is_higher_better else 'ASC'\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{metric_name} {sort_order}\"],\n",
    "        max_results=n\n",
    "    )\n",
    "    return runs\n",
    "\n",
    "def load_pipeline_from_run(run_id, model_name):\n",
    "    \"\"\"\n",
    "    Load a pipeline artifact from an MLflow run.\n",
    "\n",
    "    Args:\n",
    "    run_id (str): The run ID to load the pipeline from.\n",
    "    model_name (str): The name of the pipeline artifact.\n",
    "\n",
    "    Returns:\n",
    "    The loaded pipeline.\n",
    "    \"\"\"\n",
    "    model_uri = f\"runs:/{run_id}/{model_name}\"\n",
    "\n",
    "    if model_name == \"XGBClassifier\":\n",
    "        pipeline = mlflow.xgboost.load_model(model_uri)\n",
    "    else:\n",
    "        pipeline = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def recreate_dataframe_mapper(original_mapper):\n",
    "    \"\"\"\n",
    "    Recreate a DataFrameMapper with the same parameters and settings.\n",
    "\n",
    "    Args:\n",
    "    original_mapper (DataFrameMapper): The original DataFrameMapper.\n",
    "\n",
    "    Returns:\n",
    "    The new DataFrameMapper with the same parameters and settings.\n",
    "    \"\"\"\n",
    "    return DataFrameMapper(\n",
    "        original_mapper.features,\n",
    "        input_df=original_mapper.input_df,\n",
    "        df_out=original_mapper.df_out\n",
    "    )\n",
    "\n",
    "def recreate_pipeline_steps(original_pipeline):\n",
    "    \"\"\"\n",
    "    Recreate the pipeline steps with untrained instances.\n",
    "\n",
    "    Args:\n",
    "    original_pipeline (Pipeline): The original pipeline.\n",
    "\n",
    "    Returns:\n",
    "    A list of steps with untrained instances.\n",
    "    \"\"\"\n",
    "    new_steps = []\n",
    "    for name, step in original_pipeline.steps:\n",
    "        if isinstance(step, DataFrameMapper):\n",
    "            new_step = recreate_dataframe_mapper(step)\n",
    "        else:\n",
    "            new_step = step.__class__()\n",
    "        new_steps.append((name, new_step))\n",
    "    return new_steps\n",
    "\n",
    "def create_fresh_pipeline(original_pipeline):\n",
    "    \"\"\"\n",
    "    Create a new pipeline with the same steps and given parameters.\n",
    "\n",
    "    Args:\n",
    "    original_pipeline (Pipeline): The original pipeline.\n",
    "\n",
    "    Returns:\n",
    "    A new pipeline with the same exact steps.\n",
    "    \"\"\"\n",
    "    steps = recreate_pipeline_steps(original_pipeline)\n",
    "    new_pipeline = Pipeline(steps)\n",
    "    return new_pipeline\n",
    "\n",
    "def infer_param_types(pipeline):\n",
    "    \"\"\"\n",
    "    Infer parameter types based on the model class's default parameter values.\n",
    "    \"\"\"\n",
    "    original_params = pipeline.get_params()\n",
    "    param_types = {key: type(value) for key, value in original_params.items()}\n",
    "    return param_types\n",
    "\n",
    "def convert_params_to_original_types(params, pipeline):\n",
    "    \"\"\"\n",
    "    Convert parameter values to their original types based on the model class.\n",
    "    \"\"\"\n",
    "    param_types = infer_param_types(pipeline)\n",
    "    converted_params = {}\n",
    "    \n",
    "    for param_name, value in params.items():\n",
    "        # param_name = key.split(\"__\")[-1]\n",
    "        if param_name in param_types:\n",
    "            param_type = param_types[param_name]\n",
    "            converted_params[param_name] = param_type(value)\n",
    "        else:\n",
    "            converted_params[param_name] = value\n",
    "    return converted_params\n",
    "\n",
    "\n",
    "def retrain_top_pipelines(experiment_name, metric_name, top_n, is_higher_better, X_train, y_train, X_test, y_test, artifacts):\n",
    "    \"\"\"\n",
    "    Retrain the top n pipelines based on a specified metric with new data.\n",
    "\n",
    "    Args:\n",
    "    experiment_name (str): The name of the MLflow experiment.\n",
    "    metric_name (str): The name of the metric to sort by.\n",
    "    top_n (int): The number of top pipelines to retrain.\n",
    "    X_train (pd.DataFrame): The new training features.\n",
    "    y_train (pd.Series): The new training labels.\n",
    "    X_test (pd.DataFrame): The test set for comparing our models.\n",
    "    y_test (pd.Series): The test set labels.\n",
    "\n",
    "    Returns:\n",
    "    A list of retrained pipelines.\n",
    "    \"\"\"\n",
    "    top_runs = get_best_n_runs(experiment_name, metric_name, top_n, is_higher_better = is_higher_better)\n",
    "    retrained_pipelines = []\n",
    "\n",
    "    for run in top_runs:\n",
    "        run_id = run.info.run_id\n",
    "        model_name = run.data.tags['model_name']\n",
    "        original_pipeline = load_pipeline_from_run(run_id, model_name)\n",
    "        \n",
    "        # Create a new pipeline with untrained steps and parameters\n",
    "        new_pipeline = create_fresh_pipeline(original_pipeline)\n",
    "\n",
    "        # default_params = original_pipeline.get_params()\n",
    "        # param_types = {key: type(value) for key, value in default_params.items()}\n",
    "        # print(param_types)\n",
    "\n",
    "        # Retrieve the model parameters\n",
    "        pipeline_params, non_pipeline_params = filter_dict_by_regex(run.data.params, f'^\\w+__')\n",
    "        \n",
    "        # Convert model params to their original types\n",
    "        pipeline_params = convert_params_to_original_types(pipeline_params, original_pipeline)\n",
    "        \n",
    "        # Retrain the new pipeline with the new data\n",
    "        log_pipeline_with_mlflow(model_name = model_name,\n",
    "                        model = new_pipeline,\n",
    "                        model_params = pipeline_params,\n",
    "                        preprocess_params = non_pipeline_params,\n",
    "                        artifacts = artifacts,\n",
    "                        X_train = X_train,\n",
    "                        X_val = X_test,\n",
    "                        y_train = y_train,\n",
    "                        y_val = y_test,\n",
    "                        is_validation_set_test=True,)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_df = pd.read_csv(TRAIN_PATH, encoding='Windows-1252')\n",
    "val_df = pd.read_csv(VALIDATION_PATH, encoding='Windows-1252')\n",
    "test_df = pd.read_csv(TEST_PATH, encoding='Windows-1252')\n",
    "\n",
    "# We'll train the best models on the training and validation data\n",
    "final_train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Seperating the target column    \n",
    "final_train_df, final_train_target_df, final_train_label_for_target = prep_training_datasets(final_train_df)\n",
    "test_df, test_target_df, test_label_for_target = prep_training_datasets(test_df)\n",
    "\n",
    "experiment_name = MLFLOW_EXPERIMENT_NAME\n",
    "metric_name = \"val_weighted_recall\"\n",
    "artifacts = [\n",
    "    { \n",
    "        'type': 'dict',\n",
    "        'object': final_train_label_for_target,\n",
    "        'file_name': 'NHTSA_FARS_labels_for_target.json',\n",
    "    },\n",
    "]\n",
    "\n",
    "retrain_top_pipelines(experiment_name = experiment_name,\n",
    "                      metric_name = metric_name,\n",
    "                      top_n=3,\n",
    "                      is_higher_better = True,\n",
    "                      X_train = final_train_df,\n",
    "                      y_train = final_train_target_df,\n",
    "                      X_test = test_df,\n",
    "                      y_test = test_target_df,\n",
    "                      artifacts = artifacts,)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NHTSA-FARS-MLOps-Project-r_D6DKIt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
